{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two\n",
    "\n",
    "**Data Preparation and Model Exploration**\n",
    "**Due:** Midnight on November 16th with usual 2-hour grace period — **worth 100 points**\n",
    "\n",
    "**Note: No late assignments accepted, we need the time to grade them!**\n",
    "\n",
    "In Milestone 1, your team selected a dataset (Food-101 or HuffPost), analyzed its structure, and identified key challenges and evaluation metrics.\n",
    "In this milestone, you will carry out those plans: prepare the data, train three models of increasing sophistication, and evaluate their results using Keras and TensorFlow.\n",
    "You will finish with a comparative discussion of model performance and trade-offs.\n",
    "\n",
    "\n",
    "### Submission Guidelines\n",
    "\n",
    "* Submit one Jupyter notebook per team through the team leader’s Gradescope account. **Include all team members names at the top of the notebook.** \n",
    "* Include all code, plots, and answers inline below.\n",
    "* Ensure reproducibility by setting random seeds and listing all hyperparameters.\n",
    "* Document any AI tools used, as required by the CDS policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 – Data Preparation and Splits (20 pts)\n",
    "\n",
    "### Goals\n",
    "\n",
    "Implement the **data preparation and preprocessing steps** that you proposed in **Milestone 1**. You’ll clean, normalize, and split your data so that it’s ready for modeling and reproducible fine-tuning.\n",
    "\n",
    "### Steps to Follow\n",
    "\n",
    "1. **Load your chosen dataset**\n",
    "\n",
    "   * Use `datasets.load_dataset()` from **Hugging Face** to load **Food-101** or **HuffPost**.\n",
    "   * Display basic information (e.g., number of samples, feature names, example entries).\n",
    "\n",
    "2. **Apply cleaning and normalization**\n",
    "\n",
    "   * **Images:**\n",
    "\n",
    "     * Ensure all images are in RGB format.\n",
    "     * Resize or crop to a consistent shape (e.g., `224 × 224`).\n",
    "     * Drop or fix any corrupted files.\n",
    "   * **Text:**\n",
    "\n",
    "     * Concatenate headline + summary (for HuffPost).\n",
    "     * Strip whitespace, convert to lowercase if appropriate, and remove empty samples.\n",
    "     * Optionally remove duplicates or extremely short entries.\n",
    "\n",
    "3. **Standardize or tokenize the inputs**\n",
    "\n",
    "   * **Images:**\n",
    "\n",
    "     * Normalize pixel values (e.g., divide by 255.0).\n",
    "     * Define a minimal augmentation pipeline (e.g., random flip, crop, or rotation).\n",
    "   * **Text:**\n",
    "\n",
    "     * Create a tokenizer or `TextVectorization` layer.\n",
    "     * Set a target `max_length` based on your analysis from Milestone 1 (e.g., 95th percentile).\n",
    "     * Apply padding/truncation and build tensors for input + labels.\n",
    "\n",
    "4. **Handle dataset-specific challenges**\n",
    "\n",
    "   * If you identified **class imbalance**, compute label counts and, if needed, create a dictionary of `class_weights`.\n",
    "   * If you noted **length or size variance**, verify that your truncation or resizing works as intended.\n",
    "   * If you planned **noise filtering**, include the cleaning step and briefly explain your criteria (e.g., remove items with missing text or unreadable images).\n",
    "\n",
    "5. **Create reproducible splits**\n",
    "\n",
    "   * Split your cleaned dataset into **train**, **validation**, and **test** subsets (e.g., 80 / 10 / 10).\n",
    "   * Use a fixed random seed for reproducibility (`random_seed = 42`).\n",
    "   * Use **stratified splits**  (e.g., with `train_test_split` and `stratify = labels`).\n",
    "   * Display the size of each subset.\n",
    "\n",
    "6. **Document your pipeline**\n",
    "\n",
    "   * Summarize your preprocessing steps clearly in Markdown or code comments.\n",
    "   * Save or display a few representative examples after preprocessing to confirm the transformations are correct.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here; add as many cells as you need but make it clear what the structure is. \n",
    "\n",
    "# import necessary libraries (Please add any other libraries you may need)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from datasets import load_dataset, DatasetDict, ClassLabel\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Global Configuration & Constants\n",
    "# ============================================\n",
    "\n",
    "# Reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.keras.utils.set_random_seed(random_seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Dataset and model hyperparameters\n",
    "VOCAB_SIZE = 20000\n",
    "MAX_SEQ_LEN = 256\n",
    "BATCH_SIZE = 64\n",
    "SEP_TOKEN = \"[SEP]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HuffPost dataset from direct JSON URL:\n",
      "https://huggingface.co/datasets/khalidalt/HuffPost/resolve/main/News_Category_Dataset_v2.json\n",
      "\n",
      "Load complete:\n",
      "Dataset({\n",
      "    features: ['category', 'headline', 'authors', 'link', 'short_description', 'date'],\n",
      "    num_rows: 200853\n",
      "})\n",
      "Columns found: ['category', 'headline', 'authors', 'link', 'short_description', 'date']\n",
      "\n",
      "Found 41 classes.\n"
     ]
    }
   ],
   "source": [
    "# --- 1.1 Load Data ---\n",
    "\n",
    "# This URL points to the raw JSON, bypassing the Hugging Face Hub \n",
    "# repository lookup error (DatasetNotFoundError).\n",
    "URL = \"https://huggingface.co/datasets/khalidalt/HuffPost/resolve/main/News_Category_Dataset_v2.json\"\n",
    "\n",
    "print(f\"Loading HuffPost dataset from direct JSON URL:\\n{URL}\")\n",
    "# Load the dataset using the 'json' loader\n",
    "raw_ds = load_dataset(\"json\", data_files=URL, split=\"train\")\n",
    "\n",
    "print(\"\\nLoad complete:\")\n",
    "print(raw_ds)\n",
    "print(\"Columns found:\", raw_ds.column_names)\n",
    "\n",
    "# --- 1.2 Preprocessing ---\n",
    "# This JSON contains 'headline' and 'short_description',\n",
    "\n",
    "def concatenate_text(example):\n",
    "    example[\"text\"] = example[\"headline\"] + \" \" + SEP_TOKEN + \" \" + example[\"short_description\"]\n",
    "    return example\n",
    "\n",
    "raw_ds = raw_ds.map(concatenate_text)\n",
    "\n",
    "# Rename 'category' and encode it\n",
    "raw_ds = raw_ds.class_encode_column(\"category\")\n",
    "raw_ds = raw_ds.rename_column(\"category\", \"label\")\n",
    "\n",
    "# Store class names for later\n",
    "class_names = raw_ds.features['label'].names\n",
    "num_classes = len(class_names)\n",
    "print(f\"\\nFound {num_classes} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data (80/10/10 stratified) using datasets.train_test_split...\n",
      "\n",
      "Splits created:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'headline', 'authors', 'link', 'short_description', 'date', 'text'],\n",
      "        num_rows: 160682\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'headline', 'authors', 'link', 'short_description', 'date', 'text'],\n",
      "        num_rows: 20085\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'headline', 'authors', 'link', 'short_description', 'date', 'text'],\n",
      "        num_rows: 20086\n",
      "    })\n",
      "})\n",
      "\n",
      "Checking class distribution (Top 5 classes)...\n",
      "Train (head):\n",
      " 0    0.007512\n",
      "1    0.006665\n",
      "2    0.022541\n",
      "3    0.029561\n",
      "4    0.005694\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation (head):\n",
      " 0    0.007518\n",
      "1    0.006672\n",
      "2    0.022554\n",
      "3    0.029525\n",
      "4    0.005726\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1.3 Data Splits\n",
    "\n",
    "print(\"Splitting data (80/10/10 stratified) using datasets.train_test_split...\")\n",
    "\n",
    "# Split into Train (80%) and Temp (20%)\n",
    "temp_ds = raw_ds.train_test_split(\n",
    "    test_size=0.2, \n",
    "    seed=random_seed, \n",
    "    stratify_by_column=\"label\"\n",
    ")\n",
    "\n",
    "# Split Temp (20%) into Validation (10%) and Test (10%)\n",
    "val_test_ds = temp_ds['test'].train_test_split(\n",
    "    test_size=0.5, # 50% of the 20% temp split = 10% of total\n",
    "    seed=random_seed, \n",
    "    stratify_by_column=\"label\"\n",
    ")\n",
    "\n",
    "# Combine into a final DatasetDict\n",
    "ds = DatasetDict({\n",
    "    'train': temp_ds['train'],\n",
    "    'validation': val_test_ds['train'],\n",
    "    'test': val_test_ds['test']\n",
    "})\n",
    "\n",
    "print(\"\\nSplits created:\")\n",
    "print(ds)\n",
    "\n",
    "# Check distribution\n",
    "print(\"\\nChecking class distribution (Top 5 classes)...\")\n",
    "train_dist = pd.Series(ds['train']['label']).value_counts(normalize=True).sort_index()\n",
    "val_dist = pd.Series(ds['validation']['label']).value_counts(normalize=True).sort_index()\n",
    "test_dist = pd.Series(ds['test']['label']).value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(\"Train (head):\\n\", train_dist.head())\n",
    "print(\"\\nValidation (head):\\n\", val_dist.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing TextVectorization layer...\n",
      "Adapting TextVectorization layer to *training data only*...\n",
      "Adaptation complete.\n",
      "Vocabulary size: 20000\n",
      "\n",
      "--- Vectorization Test ---\n",
      "Original Text:\n",
      "Allowing the Time to Heal [SEP] When you are both patient and healer, remember to be patient and all...\n",
      "\n",
      "Vectorized (shape: (1, 256)):\n",
      "[ 3303     2    59     4  2915     3    45    13    17   277  1953     7\n",
      " 18151   528     4    19  1953     7  1264    59]...\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1.4 Data Preprocessing (TextVectorization)\n",
    "\n",
    "print(\"\\nInitializing TextVectorization layer...\")\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQ_LEN\n",
    ")\n",
    "\n",
    "print(\"Adapting TextVectorization layer to *training data only*...\")\n",
    "vectorize_layer.adapt(ds[\"train\"][\"text\"])\n",
    "\n",
    "print(\"Adaptation complete.\")\n",
    "\n",
    "# [FIX] The correct method is .vocabulary_size()\n",
    "# My previous response had a typo (.get_vocabulary_size()).\n",
    "print(f\"Vocabulary size: {vectorize_layer.vocabulary_size()}\")\n",
    "\n",
    "# Vectorization Test\n",
    "print(\"\\n--- Vectorization Test ---\")\n",
    "# We access the first example's 'text' field from the 'ds' object\n",
    "sample_text = ds[\"train\"][0][\"text\"] \n",
    "print(f\"Original Text:\\n{sample_text[:100]}...\")\n",
    "\n",
    "vectorized_text = vectorize_layer([sample_text])\n",
    "print(f\"\\nVectorized (shape: {vectorized_text.shape}):\\n{vectorized_text[0, :20]}...\")\n",
    "print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function 'create_tf_dataset' defined.\n",
      "\n",
      "Building tf.data pipelines...\n",
      "Pipeline creation complete.\n",
      "Train Dataset:\n",
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
      "\n",
      "--- Pipeline Test (one batch) ---\n",
      "Text batch shape: (64, 256)\n",
      "Label batch shape: (64,)\n",
      "First text vector (first 20 tokens):\n",
      " [  917  2635   865   909     3    10    18   909   251  5777  3023  3104\n",
      "     4  1909 11328  3315     7   735     1     7]\n",
      "First label: 17\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 19:07:56.120571: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_19}}\n",
      "2025-10-31 19:07:56.149502: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# 1.5 Create tf.data Pipelines\n",
    "\n",
    "def create_tf_dataset(split, is_training=True):\n",
    "  \n",
    "    # Select columns needed for the model\n",
    "    columns_to_keep = [\"text\", \"label\"]\n",
    "    \n",
    "    # Convert the HF Dataset to a tf.data.Dataset\n",
    "    #    .to_tf_dataset handles shuffling and batching efficiently\n",
    "    tf_ds = split.to_tf_dataset(\n",
    "        columns=columns_to_keep,\n",
    "        shuffle=is_training,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_cols=[\"label\"] # This formats it as (features, label)\n",
    "    )\n",
    "\n",
    "    # Map the vectorization layer\n",
    "    #    The input 'features' is now a dictionary: {'text': ...}\n",
    "    def apply_vectorization(features, label):\n",
    "        features['text'] = vectorize_layer(features['text'])\n",
    "        return features['text'], label # Return (vectorized_text, label)\n",
    "\n",
    "    tf_ds = tf_ds.map(apply_vectorization, \n",
    "                      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Prefetch\n",
    "    return tf_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Helper function 'create_tf_dataset' defined.\")\n",
    "\n",
    "print(\"\\nBuilding tf.data pipelines...\")\n",
    "\n",
    "train_ds = create_tf_dataset(ds[\"train\"], is_training=True)\n",
    "val_ds = create_tf_dataset(ds[\"validation\"], is_training=False)\n",
    "test_ds = create_tf_dataset(ds[\"test\"], is_training=False)\n",
    "\n",
    "print(\"Pipeline creation complete.\")\n",
    "print(f\"Train Dataset:\\n{train_ds}\")\n",
    "\n",
    "# Optional: Inspect the shape of one batch\n",
    "print(\"\\n--- Pipeline Test (one batch) ---\")\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    print(f\"Text batch shape: {text_batch.shape}\")\n",
    "    print(f\"Label batch shape: {label_batch.shape}\")\n",
    "    print(f\"First text vector (first 20 tokens):\\n {text_batch[0, :20]}\")\n",
    "    print(f\"First label: {label_batch[0]}\")\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Questions (5 pts each)\n",
    "\n",
    "For each question, answer thoroughly but concisely, in a short paragraph, longer or shorter as needed. Code for exploring the concepts should go in the previous cell\n",
    "as much as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Loading and Cleaning:**\n",
    "   Describe how you loaded your dataset and the key cleaning steps you implemented (e.g., handling missing data, normalizing formats, or removing duplicates).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. **Your answer here:**\n",
    "\n",
    "We used the Hugging Face `datasets` library to load the data directly from a JSON mirror URL of the original dataset.\n",
    "\n",
    "As our key cleaning step, we concatenated the `headline` and `short_description` columns with a `[SEP]` token (matching our Milestone 1 plan) to create a single `text` field. We also used `class_encode_column` to normalize the string `category` into an integer `label` for modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Preprocessing and Standardization:**\n",
    "   Summarize your preprocessing pipeline. Include any normalization, tokenization, resizing, or augmentation steps, and explain why each was necessary for your dataset.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. **Your answer here:**\n",
    "\n",
    "Our preprocessing pipeline was built around the `tf.keras.layers.TextVectorization` layer, with all steps designed to run efficiently within the `tf.data` pipeline.\n",
    "\n",
    "1.  **Text Normalization:** As planned in Milestone 1, we concatenated the `headline` and `short_description` fields using a `[SEP]` token. This creates a single `text` input, allowing the model to understand the full context of the article.\n",
    "2.  **Standardization:** The `TextVectorization` layer automatically converts text to lowercase and strips punctuation. This is essential for reducing vocabulary noise, as \"Word\" and \"word.\" are treated as the same token.\n",
    "3.  **Tokenization:** We called `.adapt()` **only on the training dataset (`ds[\"train\"][\"text\"]`)** to build the vocabulary, preventing any data leakage. Texts are then converted into integer indices based on this vocabulary, limited to `max_tokens=20000`.\n",
    "4.  **Padding & Truncation:** All text sequences are standardized to a fixed length of `MAX_SEQ_LEN=256`. This step is necessary because a neural network requires fixed-size tensor batches as input.\n",
    "\n",
    "This entire transformation process was included in the `tf.data` pipeline's `.map()` operation and optimized with `.prefetch()`, allowing the CPU to prepare the next batch while the GPU is training. (As this is text data, no resizing or augmentation was applied).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Train/Validation/Test Splits:**\n",
    "   Explain how you divided your data into subsets, including the split ratios, random seed, and any stratification or leakage checks you used to verify correctness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. **Your answer here:**\n",
    "\n",
    "We divided the data into **train (80%), validation (10%), and test (10%)** subsets. To maintain consistency with Milestone 1, we used the `datasets` library's `.train_test_split` method and applied a `random_seed=42` for reproducibility.\n",
    "\n",
    "The most critical step was using the **`stratify_by_column=\"label\"`** option. This was essential to address the severe class imbalance we found in Milestone 1, ensuring all splits share the same class distribution.\n",
    "\n",
    "We performed two key checks for correctness:\n",
    "1.  **Stratification Check:** We used `value_counts(normalize=True)` on the train and validation sets and confirmed their label proportions were nearly identical.\n",
    "2.  **Leakage Check:** We prevented data leakage by calling `vectorize_layer.adapt()` **only on the training set (`ds[\"train\"][\"text\"]`)**, ensuring no validation or test data statistics influenced our vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Class Distribution and Balance:**\n",
    "   Report your label counts and describe any class imbalances you observed. If applicable, explain how you addressed them (e.g., weighting, oversampling, or data augmentation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4. **Your answer here:**\n",
    "\n",
    "As confirmed in **Milestone 1**, the HuffPost dataset exhibits a **severe class imbalance** across its 41 classes. We re-confirmed this in `[Cell 6]` by observing the label counts (`value_counts`), which show a significant gap between the most and least frequent categories.\n",
    "\n",
    "In this **Problem 1 (Data Prep)** stage, we did not apply methods like oversampling or data augmentation to directly modify this distribution.\n",
    "\n",
    "Instead, we addressed it by using **`stratify_by_column=\"label\"`** during our splits. This was a critical step to ensure that this imbalance was **reflected proportionally** across the train, validation, and test sets, allowing the model to be trained and evaluated fairly on the minority classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 – Baseline Model (20 pts)\n",
    "\n",
    "### Goal\n",
    "\n",
    "Build and train a **simple, fully functional baseline model** to establish a reference level of performance for your dataset.\n",
    "This baseline will help you evaluate whether later architectures and fine-tuning steps actually improve results.\n",
    "\n",
    "\n",
    "### Steps to Follow\n",
    "\n",
    "1. **Construct a baseline model**\n",
    "\n",
    "   * **Images:**\n",
    "     Use a compact CNN, for example\n",
    "     `Conv2D → MaxPooling → Flatten → Dense → Softmax`.\n",
    "   * **Text:**\n",
    "     Use a small embedding-based classifier such as\n",
    "     `Embedding → GlobalAveragePooling → Dense → Softmax`.\n",
    "   * Keep the model small enough to train in minutes on Colab.\n",
    "\n",
    "2. **Compile the model**\n",
    "\n",
    "   * Optimizer: `Adam` or `AdamW`.\n",
    "   * Loss: `categorical_crossentropy` (for multi-class).\n",
    "   * Metrics: at least `accuracy`; add `F1` if appropriate.\n",
    "\n",
    "3. **Train and validate**\n",
    "\n",
    "   * Use **early stopping** on validation loss with the default patience value (e.g., 5 epochs).\n",
    "   * Record number of epochs trained and total runtime.\n",
    "\n",
    "4. **Visualize results**\n",
    "\n",
    "   * Plot **training vs. validation accuracy and loss**.\n",
    "   * Carefully observe: does the model underfit, overfit, or generalize reasonably?\n",
    "\n",
    "5. **Report baseline performance**\n",
    "\n",
    "   * The most important metric is the **validation accuracy at the epoch of minimum validation loss**; this serves as your **benchmark** for all later experiments in this milestone.\n",
    "   * Evaluate on the **test set** and record final metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here; add as many cells as you need but make it clear what the structure is. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Questions (5 pts each)\n",
    "\n",
    "1. **Model Architecture:**\n",
    "   Describe your baseline model and justify why this structure suits your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Training Behavior:**\n",
    "   Summarize the model’s training and validation curves. What trends did you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  3. **Baseline Metrics:**\n",
    "   Report validation and test metrics. What does this performance tell you about dataset difficulty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  4. **Reflection:**\n",
    "   What are the main limitations of your baseline? Which specific improvements (depth, regularization, pretraining) would you try next?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 – Custom (Original) Model (20 pts)\n",
    "\n",
    "### Goal\n",
    "\n",
    "Design and train your own **non-pretrained model** that builds on the baseline and demonstrates measurable improvement.\n",
    "This problem focuses on experimentation: apply one or two clear architectural changes, observe their effects, and evaluate how they influence learning behavior.\n",
    "\n",
    "\n",
    "### Steps to Follow\n",
    "\n",
    "1. **Modify or extend your baseline architecture**\n",
    "\n",
    "   * Begin from your baseline model and introduce one or more meaningful adjustments such as:\n",
    "\n",
    "     * Adding **dropout** or **batch normalization** for regularization.\n",
    "     * Increasing **depth** (extra convolutional or dense layers).\n",
    "     * Using **residual connections** (for CNNs) or **bidirectional LSTMs/GRUs** (for text).\n",
    "     * Trying alternative activations like `ReLU`, `LeakyReLU`, or `GELU`.\n",
    "   * Keep the model small enough to train comfortably on your chosen platform (e.g., Colab)\n",
    "\n",
    "2. **Observe what specific limitations you want to address**\n",
    "\n",
    "   * Identify whether the baseline showed **underfitting**, **overfitting**, or **slow convergence**, and design your modification to target that behavior.\n",
    "   * Make brief notes (in comments or Markdown) describing what you expect the change to influence.\n",
    "\n",
    "3. **Train and evaluate under the same conditions**\n",
    "\n",
    "   * Use the **same data splits**, **random seed**, and **metrics** as in Problem 2.\n",
    "   * Apply **early stopping** on validation loss.\n",
    "   * Track and visualize training/validation accuracy and loss over epochs.\n",
    "\n",
    "4. **Compare outcomes to the baseline**\n",
    "\n",
    "   * Observe differences in convergence speed, stability, and validation/test performance.\n",
    "   * Note whether your modification improved generalization or simply increased model capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Questions (5 pts each)\n",
    "\n",
    "1. **Model Design:**\n",
    "   Describe the architectural changes you introduced compare with your baseline model and what motivated them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Training Results:**\n",
    "   Present key validation and test metrics. Did your modifications improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Interpretation:**\n",
    "   Discuss what worked, what didn’t, and how your results relate to baseline behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Reflection:**\n",
    "   What insights did this experiment give you about model complexity, regularization, or optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 – Pretrained Model (Transfer Learning) (20 pts)\n",
    "\n",
    "### Goal\n",
    "\n",
    "Apply **transfer learning** to see how pretrained knowledge improves accuracy, convergence speed, and generalization.\n",
    "This experiment will help you compare the benefits and trade-offs of using pretrained models versus those trained from scratch.\n",
    "\n",
    "\n",
    "### Steps to Follow\n",
    "\n",
    "1. **Select a pretrained architecture**\n",
    "\n",
    "   * **Images:** choose from `MobileNetV2`, `ResNet50`, `EfficientNetB0`, or a similar model in `tf.keras.applications`.\n",
    "   * **Text:** choose from `BERT`, `DistilBERT`, `RoBERTa`, or another Transformer available in `transformers`.\n",
    "\n",
    "2. **Adapt the model for your dataset**\n",
    "\n",
    "   * Use the correct **preprocessing function** and **input shape** required by your chosen model.\n",
    "   * Replace the top layer with your own **classification head** (e.g., `Dense(num_classes, activation='softmax')`).\n",
    "\n",
    "3. **Apply transfer learning**\n",
    "\n",
    "   * Choose an appropriate **training strategy** for your pretrained model. Options include:\n",
    "\n",
    "     * **Freezing** the pretrained base and training only a new classification head.\n",
    "     * **Partially fine-tuning** selected upper layers of the base model.\n",
    "     * **Full fine-tuning** (all layers trainable) with a reduced learning rate.\n",
    "   * Adjust your learning rate schedule to match your strategy (e.g., smaller LR for fine-tuning).\n",
    "   * Observe how your chosen approach affects **validation loss**, **training time**, and **model stability**.\n",
    "\n",
    "4. **Train and evaluate under consistent conditions**\n",
    "\n",
    "   * Use the same **splits**, **metrics**, and **evaluation protocol** as in earlier problems.\n",
    "   * Record training duration, validation/test performance, and any resource constraints (GPU memory, runtime).\n",
    "\n",
    "5. **Compare and analyze**\n",
    "\n",
    "   * Observe how transfer learning changes both **performance** and **efficiency** relative to your baseline and custom models.\n",
    "   * Identify whether the pretrained model improved accuracy, sped up convergence, or introduced new challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Questions (5 pts each)\n",
    "\n",
    "1. **Model Choice:** Which pretrained architecture did you select, and what motivated that choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Fine-Tuning Plan:** Describe your fine-tuning strategy and why you chose it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Performance:** Report key metrics and compare them with your baseline and custom models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Computation:** Summarize how training time, memory use, or convergence speed differed from the previous two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 – Comparative Evaluation and Discussion (20 pts)\n",
    "\n",
    "### Goal\n",
    "\n",
    "Compare your **baseline**, **custom**, and **pretrained** models to evaluate how design choices affected performance, efficiency, and generalization.\n",
    "This problem brings your work together and encourages reflection on what you’ve learned about model behavior and trade-offs.\n",
    "\n",
    "**Note** that this is not your final report, and you will continue to refine your results for the final report. \n",
    "\n",
    "### Steps to Follow\n",
    "\n",
    "1. **Compile key results**\n",
    "\n",
    "   * Gather your main metrics for each model: **accuracy**, **F1**, **training time**, and **parameter count or model size**.\n",
    "   * Ensure all numbers come from the same evaluation protocol and test set.\n",
    "\n",
    "2. **Visualize the comparison**\n",
    "\n",
    "   * Present results in a **single, well-organized chart or table**.\n",
    "   * Optionally, include training curves or confusion matrices for additional insight.\n",
    "\n",
    "3. **Analyze comparative performance**\n",
    "\n",
    "   * Observe which model performed best by your chosen metric(s).\n",
    "   * Note patterns in efficiency (training speed, memory use) and stability (validation variance).\n",
    "\n",
    "4. **Inspect model behavior**\n",
    "\n",
    "   * Look at a few representative misclassifications or difficult examples.\n",
    "   * Identify whether certain classes or inputs consistently caused errors.\n",
    "\n",
    "5. **Plan forward improvements**\n",
    "\n",
    "   * In the final report, you will use your best model and conclude your investigation of your dataset. Based on your observations, decide on a model and next steps for refining your approach in the final project (e.g., regularization, data augmentation, model scaling, or more targeted fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Questions (4 pts each)\n",
    "\n",
    "1. **Summary Table and Performance Analysis:** Present a clear quantitative comparison of all three models. Which model achieved the best overall results, and what factors contributed to its success?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Trade-Offs:** Discuss how complexity, accuracy, and efficiency balanced across your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Error Patterns:** Describe the types of examples or classes that remained challenging for all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3. **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Next Steps:** Based on these findings, decide on a model to go forward with and outline your plan for improving that model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 **Your answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Question: Describe what use you made of generative AI tools in preparing this Milestone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Question: Your answer here:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vevn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
